{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90ab5a-a3fd-40b4-b803-7c945ba3f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium\n",
    "#!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5c7d6b-2926-43be-8a6c-c54ef5ca0eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)\n",
    "print(env.action_space)\n",
    "\n",
    "\n",
    "valores = [0.0]\n",
    "\n",
    "for _ in range(300):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    valores.append(reward)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44318499-d723-4a3e-84ee-21a45a203c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dpg\n",
    "\n",
    "#%run dpg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4d62b2-33cb-4c67-a611-57d11cf02b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85222542-3ce9-44aa-98e7-edef5bc151f1",
   "metadata": {},
   "source": [
    "# Código de prueba MPC 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f2fefe-2406-4467-ba49-e809854d5a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Optional\n",
    "\n",
    "import casadi as cs\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from csnlp import Nlp\n",
    "from csnlp.wrappers import Mpc\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "from mpcrl import (\n",
    "    LearnableParameter,\n",
    "    LearnableParametersDict,\n",
    "    LstdDpgAgent,\n",
    "    UpdateStrategy,\n",
    ")\n",
    "from mpcrl import exploration as E\n",
    "from mpcrl.optim import GradientDescent\n",
    "from mpcrl.util.control import dlqr\n",
    "from mpcrl.wrappers.agents import Log, RecordUpdates\n",
    "from mpcrl.wrappers.envs import MonitorEpisodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daefc2f-c66d-4bae-8247-710cb159c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleSystem:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        #self.env = gym.make(\"CartPole-v1\", m_p=0.5, m_c=1.0, length=0.5)\n",
    "        #self.env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def get_stage_cost(self, state):\n",
    "        # Define the individual cost components\n",
    "        x, x_dot, theta, theta_dot = state  # Extract state variables\n",
    "\n",
    "        # Penalty for being far from the center position (cart position)\n",
    "        cart_pos_cost = abs(x)\n",
    "\n",
    "        # Penalty for high angular velocity\n",
    "        theta_dot_cost = abs(theta_dot)\n",
    "\n",
    "        # Penalty for being far from upright (cart angle)\n",
    "        pole_angle_cost = abs(theta)\n",
    "\n",
    "        # Penalty for high cart velocity\n",
    "        x_dot_cost = abs(x_dot) if pole_angle_cost < np.pi/2 else 0  # Penalize only when pole is upright\n",
    "\n",
    "        # Combine individual costs with weights (adjust weights as needed)\n",
    "        stage_cost = 0.1 * cart_pos_cost + 0.2 * theta_dot_cost + \\\n",
    "                     0.5 * pole_angle_cost + 0.1 * x_dot_cost\n",
    "\n",
    "        return stage_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bed638-3122-4852-bed7-9931c045ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleMpc(Mpc[cs.SX]):\n",
    "\n",
    "    m_p=0.5\n",
    "    m_c=1.0, \n",
    "    length=0.5\n",
    "    horizon = 10                                        # Horizonte de predicción, pasos hacia el futuro a predecir\n",
    "    discount_factor = 0.9                               # Para ponderar la importancia de las recompensas futuras en relación con las recompensas inmediatas\n",
    "    learnable_pars_init = {                             # Parámetros que se pueden aprender durante el entrenamiento del controlador.\n",
    "        \"V0\": np.asarray([0, 0]),                            # Valor inicial \n",
    "        \"x_lb\": np.asarray([-2.4, 0]),                          # Son los límites inferiores y superiores para el estado del sistema.\n",
    "        \"x_ub\": np.asarray([2.4, 0]),\n",
    "        \"b\": np.zeros(4),\n",
    "        \"f\": np.zeros(4+1),\n",
    "        \"A\": np.asarray([[0, 1, 0, 0], [0, 0, m_p*9.8/m_c, 0], [0, 0, 0, 1], [0, 0, (m)/, 0]]),\n",
    "        \"B\": np.asarray([[0.0312], [0.25]]),\n",
    "    }\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        N = self.horizon\n",
    "        gamma = self.discount_factor\n",
    "        w = LtiSystem.w\n",
    "        nx, nu = LtiSystem.nx, LtiSystem.nu\n",
    "        x_bnd, a_bnd = LtiSystem.x_bnd, LtiSystem.a_bnd\n",
    "        nlp = Nlp[cs.SX]()\n",
    "        super().__init__(nlp, N)\n",
    "\n",
    "        # parameters\n",
    "        V0 = self.parameter(\"V0\", (nx,))\n",
    "        x_lb = self.parameter(\"x_lb\", (nx,))\n",
    "        x_ub = self.parameter(\"x_ub\", (nx,))\n",
    "        b = self.parameter(\"b\", (nx, 1))\n",
    "        f = self.parameter(\"f\", (nx + nu, 1))\n",
    "        A = self.parameter(\"A\", (nx, nx))\n",
    "        B = self.parameter(\"B\", (nx, nu))\n",
    "\n",
    "        # variables (state, action, slack)                #########################\n",
    "        x, _ = self.state(\"x\", nx, bound_initial=False)\n",
    "        u, _ = self.action(\"u\", nu, lb=a_bnd[0], ub=a_bnd[1])\n",
    "        s, _, _ = self.variable(\"s\", (nx, N), lb=0)\n",
    "\n",
    "        # dynamics\n",
    "        self.set_dynamics(lambda x, u: A @ x + B * u + b, n_in=2, n_out=1)\n",
    "        self.set_dynamics(self, x, u):\n",
    "            \n",
    "\n",
    "        # other constraints\n",
    "        self.constraint(\"x_lb\", x_bnd[0] + x_lb - s, \"<=\", x[:, 1:])\n",
    "        self.constraint(\"x_ub\", x[:, 1:], \"<=\", x_bnd[1] + x_ub + s)\n",
    "\n",
    "        # objective\n",
    "        A_init, B_init = self.learnable_pars_init[\"A\"], self.learnable_pars_init[\"B\"]\n",
    "        S = cs.DM(dlqr(A_init, B_init, 0.5 * np.eye(nx), 0.25 * np.eye(nu))[1])\n",
    "        gammapowers = cs.DM(gamma ** np.arange(N)).T\n",
    "        self.minimize(\n",
    "            V0.T @ x[:, 0]  # to have a derivative, V0 must be a function of the state\n",
    "            + cs.bilin(S, x[:, -1])\n",
    "            + cs.sum2(f.T @ cs.vertcat(x[:, :-1], u))\n",
    "            + 0.5\n",
    "            * cs.sum2(\n",
    "                gammapowers\n",
    "                * (cs.sum1(x[:, :-1] ** 2) + 0.5 * cs.sum1(u**2) + w.T @ s)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # solver\n",
    "        opts = {\n",
    "            \"expand\": True,\n",
    "            \"print_time\": False,\n",
    "            \"bound_consistency\": True,\n",
    "            \"clip_inactive_lam\": True,\n",
    "            \"calc_lam_x\": False,\n",
    "            \"calc_lam_p\": False,\n",
    "            \"jit\": False,\n",
    "            \"ipopt\": {\n",
    "                # \"linear_solver\": \"pardiso\",\n",
    "                # \"tol\": 1e-5,\n",
    "                # \"barrier_tol_factor\": 1,\n",
    "                \"max_iter\": 500,\n",
    "                \"sb\": \"yes\",\n",
    "                \"print_level\": 0,\n",
    "            },\n",
    "        }\n",
    "        self.init_solver(opts, solver=\"ipopt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777816b8-72cd-4776-8177-72a5e003306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's create the instances of such classes and start the training\n",
    "#mpc = LinearMpc()\n",
    "#learnable_pars = LearnableParametersDict[cs.SX](\n",
    "#    (\n",
    "#        LearnableParameter(name, val.shape, val, sym=mpc.parameters[name])\n",
    "#        for name, val in mpc.learnable_pars_init.items()\n",
    "#    )\n",
    "#)\n",
    "\n",
    "\n",
    "### env = MonitorEpisodes(TimeLimit(LtiSystem(), max_episode_steps=int(5e3)))\n",
    "env = MonitorEpisodes(TimeLimit(gym.make(\"CartPole-v1\"), max_episode_steps=500))\n",
    "\n",
    "mpc = CartPoleMpc()\n",
    "\n",
    "rollout_length = 100\n",
    "agent = Log(\n",
    "    RecordUpdates(\n",
    "        LstdDpgAgent(\n",
    "            mpc=mpc,\n",
    "            learnable_parameters=learnable_pars,\n",
    "            discount_factor=mpc.discount_factor,\n",
    "            optimizer=GradientDescent(learning_rate=1e-6),\n",
    "            update_strategy=UpdateStrategy(rollout_length, \"on_timestep_end\"),\n",
    "            rollout_length=rollout_length,\n",
    "            exploration=E.GreedyExploration(0.05),\n",
    "            record_policy_performance=True,\n",
    "            record_policy_gradient=True,\n",
    "        )\n",
    "    ),\n",
    "    level=logging.DEBUG,\n",
    "    log_frequencies={\"on_timestep_end\": 1000},\n",
    ")\n",
    "agent.train(env=env, episodes=1, seed=69)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a5bd69-6156-4d48-b602-bf6797a9e77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3d6485f-2234-43b5-bfd9-ee3db2871382",
   "metadata": {},
   "source": [
    "# Código de prueba MPC 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c5f458f-2fa6-4d9b-b77f-90c1b4890fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleSystem(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode=mode)\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def get_stage_cost(self, state):\n",
    "        # Define the individual cost components\n",
    "        x, x_dot, theta, theta_dot = state  # Extract state variables\n",
    "\n",
    "        # Penalty for being far from the center position (cart position)\n",
    "        cart_pos_cost = abs(x)\n",
    "\n",
    "        # Penalty for high angular velocity\n",
    "        theta_dot_cost = abs(theta_dot)\n",
    "\n",
    "        # Penalty for being far from upright (cart angle)\n",
    "        pole_angle_cost = abs(theta)\n",
    "\n",
    "        # Penalty for high cart velocity\n",
    "        x_dot_cost = abs(x_dot) if pole_angle_cost < np.pi/2 else 0  # Penalize only when pole is upright\n",
    "\n",
    "        # Combine individual costs with weights (adjust weights as needed)\n",
    "        stage_cost = 0.1 * cart_pos_cost + 0.2 * theta_dot_cost + \\\n",
    "                     0.5 * pole_angle_cost + 0.1 * x_dot_cost\n",
    "        pass\n",
    "\n",
    "\n",
    "class CartPoleMpc(Mpc[cs.SX]):\n",
    "    def __init__(self, system):\n",
    "        self.system = system  # Instancia de CartPoleSystem\n",
    "        # Inicializa cualquier otro parámetro necesario para MPC\n",
    "        m_p=0.5\n",
    "        m_c=1.0 \n",
    "        length=0.5\n",
    "        g = 9.8\n",
    "        # Definir las matrices A, B y el vector b\n",
    "        #self.A = np.eye([[0, 1, 0, 0],[0, 0, (m_p*g)/m_c, 0],[0, 0, 0, 1],[0, 0, ((m_c+m_p)*g)/(m_c*length), 0]])  # Matriz de transición de estado (ya definida)\n",
    "        self.A = np.eye(4)\n",
    "        self.B = np.array([0, 1/m_c, 0, -1/(m_c * length)])  # Vector de entrada (acción)\n",
    "        self.b = np.zeros(4)  # Término de sesgo (puedes ajustarlo según tu modelo)\n",
    "        super()._init_(system)\n",
    "\n",
    "    def dynamics(self, x, u):\n",
    "        \"\"\"\n",
    "        Ecuaciones de dinámica del sistema.\n",
    "        x: Estado (vector de 4 dimensiones)\n",
    "        u: Acción (fuerza aplicada al carrito)\n",
    "        \"\"\"\n",
    "        return self.A @ x + self.B * u + self.b\n",
    "    \n",
    "    def plan(self, current_state):\n",
    "        # Implementa la lógica para planificar la secuencia de acciones\n",
    "        # Esto puede incluir la predicción del estado futuro y la optimización del costo\n",
    "        #pass\n",
    "        \"\"\"\n",
    "        Planifica la secuencia de acciones basada en el estado actual.\n",
    "\n",
    "        Args:\n",
    "            current_state (np.ndarray): Vector de estado actual (por ejemplo, [x, x_dot, theta, theta_dot]).\n",
    "\n",
    "        Returns:\n",
    "            action (float): Acción planificada (por ejemplo, fuerza aplicada al carrito).\n",
    "        \"\"\"\n",
    "        # Implementa aquí tu lógica de planificación:\n",
    "        # - Puedes usar algoritmos de optimización, controladores PID, etc.\n",
    "        # - Considera los objetivos de control y los costos asociados.\n",
    "\n",
    "        # Ejemplo simple: Mantener el poste vertical\n",
    "        # Calcula la acción como una función del ángulo actual del poste\n",
    "        theta = current_state[2]\n",
    "        action = -theta  # Invierte el ángulo como acción\n",
    "        return action\n",
    "\n",
    "    def execute(self):\n",
    "        # Ejecuta la secuencia de acciones planificadas en el entorno\n",
    "        state = self.system.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.plan(state)\n",
    "            state, reward, done, _ = self.system.step(action)\n",
    "            # Opcional: renderizar el entorno si es necesario\n",
    "            self.system.render()\n",
    "        self.system.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "756d3242-0bb6-429c-a53d-d2366f6adadc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '_init_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Crear la instancia de la nueva clase MPC\u001b[39;00m\n\u001b[0;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m MonitorEpisodes(TimeLimit(CartPoleSystem(), max_episode_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m5e3\u001b[39m)))\n\u001b[1;32m----> 5\u001b[0m mpc \u001b[38;5;241m=\u001b[39m \u001b[43mCartPoleMpc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#cartpole_env = CartPoleSystem()\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#mpc_agent = CartPoleMpc(cartpole_env)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#        for name, val in mpc.learnable_pars_init.items()\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m learnable_pars \u001b[38;5;241m=\u001b[39m LearnableParametersDict[cs\u001b[38;5;241m.\u001b[39mSX](\n\u001b[0;32m     17\u001b[0m     (\n\u001b[0;32m     18\u001b[0m         LearnableParameter(name, val\u001b[38;5;241m.\u001b[39mshape, val, sym\u001b[38;5;241m=\u001b[39mmpc\u001b[38;5;241m.\u001b[39mparameters[name])\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m mpc\u001b[38;5;241m.\u001b[39mlearnable_pars_init\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     20\u001b[0m     )\n\u001b[0;32m     21\u001b[0m )\n",
      "Cell \u001b[1;32mIn[11], line 57\u001b[0m, in \u001b[0;36mCartPoleMpc.__init__\u001b[1;34m(self, system)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm_c, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(m_c \u001b[38;5;241m*\u001b[39m length)])  \u001b[38;5;66;03m# Vector de entrada (acción)\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# Término de sesgo (puedes ajustarlo según tu modelo)\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_\u001b[49m(system)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'super' object has no attribute '_init_'"
     ]
    }
   ],
   "source": [
    "# Crear la instancia de la nueva clase MPC\n",
    "\n",
    "env = MonitorEpisodes(TimeLimit(CartPoleSystem(), max_episode_steps=int(5e3)))\n",
    "\n",
    "mpc = CartPoleMpc(system = env)\n",
    "\n",
    "\n",
    "#cartpole_env = CartPoleSystem()\n",
    "#mpc_agent = CartPoleMpc(cartpole_env)\n",
    "\n",
    "# Definir los parámetros aprendibles para el nuevo MPC\n",
    "#learnable_pars = LearnableParametersDictcs.SX\n",
    "#        for name, val in mpc.learnable_pars_init.items()\n",
    "#)\n",
    "\n",
    "learnable_pars = LearnableParametersDict[cs.SX](\n",
    "    (\n",
    "        LearnableParameter(name, val.shape, val, sym=mpc.parameters[name])\n",
    "        for name, val in mpc.learnable_pars_init.items()\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Crear el entorno utilizando CartPoleSystem\n",
    "#env = MonitorEpisodes(TimeLimit(CartPoleSystem(), max_episode_steps=int(5e3)))\n",
    "\n",
    "# Definir la longitud del rollout\n",
    "rollout_length = 100\n",
    "\n",
    "# Crear y configurar el agente con la nueva clase MPC y los parámetros actualizados\n",
    "agent = Log(\n",
    "    RecordUpdates(\n",
    "        LstdDpgAgent(\n",
    "            mpc=mpc,\n",
    "            learnable_parameters=learnable_pars,\n",
    "            discount_factor=mpc.discount_factor,\n",
    "            optimizer=GradientDescent(learning_rate=1e-6),\n",
    "            update_strategy=UpdateStrategy(rollout_length, \"on_timestep_end\"),\n",
    "            rollout_length=rollout_length,\n",
    "            exploration=E.GreedyExploration(0.05),\n",
    "            record_policy_performance=True,\n",
    "            record_policy_gradient=True,\n",
    "        )\n",
    "    ),\n",
    "    level=logging.DEBUG,\n",
    "    log_frequencies={\"on_timestep_end\": 1000},\n",
    ")\n",
    "\n",
    "# Entrenar el agente con el nuevo entorno\n",
    "agent.train(env=env, episodes=1, seed=69)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71236be2-8de0-45a9-9557-c19d7dd674aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
