%% ---------------------------------------------------------------------------
%% intro.tex
%%
%% Introduction
%%
%% $Id: intro.tex 1477 2010-07-28 21:34:43Z palvarado $
%% ---------------------------------------------------------------------------

\begin{comment}

$********************************************$\\
\textbf{Alfonso Chacón} 

\textit{Definición de problema a resolver y enfoque de solución}
Problema un poco abierto quizás pero claro. 
A como está planteado, puede disputarse si la elección de un método de aprendizaje reforzado es la óptima (algo igual no tan relevante). ¿Qué se gana con usar un método como los propuestos en un problema ya resuelto por métodos clásicos? 
¿Más allá de la obvia consderación didáctica? (o simplemente: lo hacemos porque lo podemos hacer). Considero conveniente contestar esta pregunta de manera explícita en el desarrollo del proyecto.

\textit{Objetivos y otros}
Objetivo general: Adecuado. Se puede mejorar dándole un poco más de especificidad, es decir, aclarar que el método a usar es DRL. Pero nada serio.

\textit{Redacción}
Buena. Algunos detalles de titulación mínimos (en español, solo se capitulan la primera letra del título y los nombres propios en los mismos).

\textit{Criterio: Aprobado  27/02/2024}

$********************************************$\\
\textbf{Javier Pérez}

\textit{Entorno del problema}
El Entorno del problema es muy largo, hay mucho texto que no es necesario para entender las circuinstancias del proyecto. Sea más directo.

\textit{Planteamiento del problema}
Los dos primeros párrafos del Planteamiento del problema es más parte de un marco teórico.

\textit{Soluciones}
La redacción de las posibles soluciones y Selección de la solución se vuelve confusa por momentos.
El diagrama que pone de la solución es más bien un esquema de la secuencia de pasos que se realizarán. No es lo mismo.

\textit{Criterio: Aprobado  05/02/2024}

$********************************************$\\

\end{comment}

\chapter{Introducción}
\label{chp:intro}

\section{Aprendizaje automático en control}

Los sistemas de control automático en la ingeniería buscan satisfacer criterios de optimalidad preestablecidos. Algunos ejemplos de aplicación de técnicas de control son los movimientos precisos de los brazos robóticos en una planta de ensamblaje, la autonomía de procesos en los vehículos automotores y eléctricos, el funcionamiento de los reactores químicos o nucleares, el funcionamiento de equipos médicos, entre otros, todos buscando garantizar comportamientos que optimicen el rendimiento o costo de recursos \cite{Kuo}.

Con el paso de los años el control automático ha reaccionado a cambios y demandas que la industria exige. Se busca optimizar funciones de manera que se alcancen capacidades de autonomía y flexibilidad, mientras se asegura la precisión y eficiencia de los sistemas. Sin embargo, las técnicas utilizadas en control en general logran solo una de estas capacidades. Por ejemplo, los sistemas de control programables ofrecen flexibilidad en el sistema, pero si se cambia ligeramente la configuración de la planta, el control deja de funcionar, evidenciando su falta de autonomía; y por otro lado, las automatizaciones permiten la autonomía de los equipos, pero únicamente responden a un tipo de maquinaria, por lo que no ofrecen flexibilidad \cite{Dorf}. La figura \ref{fig:Controlproj} ilustra que las nuevas tecnologías en robótica e inteligencia artificial IA buscan ofrecer las dos capacidades mencionadas y de manera consecuente, llevan a un salto de calidad, seguridad y rentabilidad \cite{Dorf} \cite{AI}.

\begin{figure}[hh]
	\centering
	\includegraphics[scale=0.4]{fig/new/Proyección.png}
	\caption{Proyección de la evolución de los sistemas de control. Fuente: \cite{Dorf}.}
	\label{fig:Controlproj}
\end{figure}

Los métodos clásicos de diseño en control, se basan a menudo en modelos matemáticos lineales, que no aplican a sistemas que presentan dinámicas no lineales o perturbaciones externas significativas. Dentro de los beneficios de utilizar IA para el control de equipos o sistemas se tiene, primero, la adaptabilidad en entornos inciertos. Las plantas a controlar, incluyendo robots, deberán operar en ambientes con obstáculos o cambios imprevisibles. Los métodos de control tradicionales se basan en respuestas preprogramadas, que pueden no funcionar en situaciones imprevistas. El aprendizaje reforzado (RL, por sus siglas en inglés) permite a ``agentes'' aprender por ensayo y error, adaptando su comportamiento en función de recompensas recibidas por acciones específicas. Esto los hace menos susceptibles a fallar a la hora de manejar lo inesperado, e incluso habilita a los sistemas a continuar aprendiendo durante su operación; pueden optimizar su comportamiento aun si su entorno cambia \cite{Cable-Driven} \cite{SoftComputing}.

Tareas usualmente invariantes junto con una programación estática, dificultan al controlador el adaptarse a situaciones imprevistas o descomponer tareas complejas en una serie de acciones más sencillas, resultando en todo un reto para el desarrollo con métodos de control tradicionales. El RL destaca en este tipo de situaciones. Aprender de las recompensas asociadas a cada acción permite que el sistema pueda desarrollar una estrategia o política para realizar trabajos de especial cuidado, incluso sin programación explícita para cada paso \cite{Cable-Driven}.

Al considerar sistemas no lineales de alta complejidad, el desarrollo de sistemas de control clásico moderno requieren conocimientos especializados, punto en el que las nuevas prácticas del RL alivianan esta carga. Al proporcionar una estructura general para el aprendizaje, el RL permite a los sistemas adquirir las habilidades necesarias a través de la interacción con el entorno \cite{SoftComputing}.


\section{Control con aprendizaje reforzado}

En el Laboratorio de Procesamiento de Imágenes y Señales (SIP-Lab) de la Escuela de Ingeniería Electrónica del ITCR se experimenta con la aplicación de RL en el control para su posterior incorporación en actividades de investigación y docencia.

En la figura \ref{fig:DiagProyecto}, se ilustran las fases del marco experimental a utilizar. Como planta física se emplea un péndulo amortiguado con motor y hélice (PAMH). Como primer paso, se evaluó una red neuronal imitadora de la planta física como parte del proyecto de graduación de Brenes Alfaro \cite{JorgeBrenes}, que resultó en un modelo con características similares a la planta real. Este modelo permite experimentar sin riesgo de dañar la planta física real durante los procesos de aprendizaje

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/new/DiagProyecto.png}
    \caption{Diagrama de etapas involucradas en la solución. En gris se muestran las etapas realizadas en \cite{JorgeBrenes}. En verde se muestran las etapas correspondientes a este trabajo y en blanco las etapas a realizar en un trabajo futuro. Adaptado de: \cite{JorgeBrenes}.}
    \label{fig:DiagProyecto}
\end{figure}

Ya que se cuenta con un modelo imitador, el siguiente paso en el desarrollo del proyecto correspondiente al entrenamiento de un modelo controlador de la planta de laboratorio. En la figura \ref{fig:DiagramaGeneral} se muestra la estructura general del sistema controlador, resaltada la sección que corresponde al presente proyecto, que se utilizará con los interruptores A y C abiertos y B cerrado. En iteraciones posteriores corresponderá desconectar B y conectar la planta real con A.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/new/DiagramaGeneralv2.png}
    \caption{Diagrama de control para la PAHM. La línea punteada de color rojo muestra el bloque a dearrollar. Fuente: \cite{JorgeBrenes}.}
    \label{fig:DiagramaGeneral}
\end{figure}



\section{Modelo controlador de planta de laboratorio PAMH con RL}

Este trabajo corresponde entonces a una segunda etapa de un proyecto de control del PAMH mediante técnicas de RL y redes neuronales artificiales. En la figura \ref{fig:DiagEtapas} se muestra la distribución de las etapas del presente trabajo. El primer paso consistie en recopilar, estudiar y calificar los modelos de RL para su selección como método a implementar. Seguidamente se entrena el modelo incorporando la implementación de un modelo RNAM virtual de la planta, con ajustes a sus  hiperparámetros. Por último, se evalúa el desempeño en el control del entorno PAMH.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/new/DiagEtapas.png}
    \caption{Etapas del entrenamiento del modelo de RL usando la red neuronal imitadora (RNAM) para simular el PAMH real. La línea punteada de color rojo muestra el bloque de implementación y entrenamiento. En gris el producto del trabajo en \cite{JorgeBrenes}.}
    \label{fig:DiagEtapas}
\end{figure}


\section{Objetivos y estructura del documento}

El objetivo general del presente proyecto es diseñar un sistema de aprendizaje automático para el control del ángulo de una planta no lineal PAMH. El primer objetivo específico se enfoca en la selección de un método de aprendizaje reforzado apto para el control no lineal. Esto último es necesario por las características del sistema PAMH. En la selección se deben tomar en cuenta los recursos y los algoritmos de RL disponibles; por lo tanto, es necesario calificar los métodos planteados en primera mano y su congruencia respecto al objetivo a cumplir.

Seguidamente, el segundo objetivo específico es implementar el método de RL seleccionado, utilizando el modelo mimetizador (RNAM) como planta a controlar. Es necesario hacer los ajustes respectivos al algoritmo y especificar la función de recompensa del método para lograr controlar el ángulo del péndulo.

Como tercer objetivo se deben evaluar los resultados del modelo de RL para el control de la RNAM. Se deben identificar las deficiencias y ventajas del caso.

El presente documento contiene en su capítulo \ref{ch:teoria} los principios teóricos del funcionamiento del aprendizaje reforzado, así como una explicación de los métodos utilizados. En el capítulo \ref{ch:ControlPAMH} se describe la secuencia de tareas realizadas para alcanzar los objetivos específicos mencionados anteriormente.

Por último, el capítulo \ref{ch:resultados} y el capítulo \ref{ch:conclusion} muestran los resultados de los procesos de entrenamiento con las valoraciones de estos y se plantean conclusiones y recomendaciones.









\begin{comment}

\section{Entorno del proyecto}

El control automático de sistemas es una rama de la ingeniería que se dedica al diseño y análisis de sistemas de control que de manera automática buscan satisfacer criterios de optimalidad preestablecidos. Estos sistemas se utilizan en aplicaciones que abarcan desde el control de procesos industriales hasta el control de sistemas de navegación en vehículos autónomos, donde para lograr un control automático efectivo, se emplean técnicas y algoritmos, como el control proporcional-integral-derivativo (PID), el control adaptativo, el control moderno y otros \textcolor{SkyBlue}{ControlModerno}. La implementación de estos sistemas requiere del uso de hardware y software especializados, así como del conocimiento en áreas de la electrónica, informática y actualmente, la aplicación de la inteligencia artificial (IA) \textcolor{SkyBlue}{Kuo}.

El campo de aplicación de la IA está en constante crecimiento, impulsado por la necesidad de automatizar procesos y mejorar la eficiencia en diversas industrias. La IA se utiliza en sistemas de control para incrementar la precisión y velocidad de respuesta, apoyando así la toma de decisiones en tiempo real \textcolor{SkyBlue}{IntroSistemasControl} \textcolor{SkyBlue}{SistemaAlmidon}. Algunas de las aplicaciones más comunes incluyen la robótica, el control de procesos industriales, la domótica y la automatización de vehículos \textcolor{SkyBlue}{MarketResearch}. De acuerdo con un informe de Allied Market Research \textcolor{SkyBlue}{MarketResearch}, se espera que el mercado global de sistemas controlados mediante IA alcance los $\$ 30.8$ mil millones para el año 2026, con una tasa de crecimiento anual compuesta del $33.7\%$ desde 2019 hasta 2026. Se espera que la creciente demanda de soluciones de automatización, la evolución de esta tecnología y la creciente inversión en investigación y desarrollo impulsen aún más el crecimiento de este mercado en los próximos años \textcolor{SkyBlue}{MarketResearch}.

El fuerte aumento en la introducción del uso de IA en diversos ámbitos del mercado mundial, obliga a las universidades, a mantenerse activas en la propuesta y mejora de aplicaciones para la IA y su correspondiente divulgación. Esto se observa en la tendencia de investigaciones de las universidades líderes en tecnología a nivel mundial, como el Massachusetts Institute of Technology (MIT), la Universidad de Stanford, la Universidad de Oxford, entre otras \textcolor{SkyBlue}{UniversidadesIA}. Los experimentos que se realizan incluyen el desarrollo de algoritmos de aprendizaje automático para analizar grandes conjuntos de datos y descubrir patrones y tendencias, la aplicación de técnicas para resolver problemas en campos tan diversos como la medicina, la ingeniería, las ciencias sociales, además de la investigación en el uso de estas herramientas para mejorar la eficacia de los sistemas educativos \textcolor{SkyBlue}{MachineLearning}. Estos proyectos no solo están ayudando a los estudiantes a adquirir habilidades valiosas y a estar mejor preparados para los desafíos del mundo laboral, sino que también están generando nuevas oportunidades de investigación y desarrollo en áreas clave. Algunos ejemplos son la utilización de redes neuronales para la predicción del rendimiento académico, detección de enfermedades, identificación de aves, reconocimiento de emociones, entre otros \textcolor{SkyBlue}{MachineLearning}.

Alineado con lo anterior, el SIPLab de la Escuela de Ingeniería Electrónica del Instituto Tecnológico de Costa Rica, busca desarrollar soluciones a problemas regionales y nacionales en el campo mencionado anteriormente, esto mediante proyectos de procesamiento de señales donde se integre el aprendizaje automático y sus aplicaciones, permitiendo que estudiantes y profesores incursionen en el tema de la inteligencia artificial y la apliquen en sus actividades academicas \textcolor{SkyBlue}{SIPLab}. 



\section{Planteamiento del problema}

\subsection{Generalidades}

En la actualidad, el incremento en la complejidad de las plantas de control y su variedad de componentes dificulta el diseño del controlador y su optimización. Una solución prometedora para este problema es la aplicación de técnicas de aprendizaje automático, específicamente el aprendizaje reforzado (RL), el cual permite que un sistema aprenda de su experiencia y adapte su comportamiento para lograr una tarea específica, esto gracias a un diseño eficiente en la toma de muestras de información, su interpretación y modelado respectivo. Este RL presenta una variedad considerable de métodos que posibilitan su clasificación en distintas categorías, siendo las principales el aprendizaje reforzado basado en un modelo y el aprendizaje reforzado sin modelo \textcolor{SkyBlue}{AprendRefor} \textcolor{SkyBlue}{DataScience}.

El RL como tal requiere tener un panorama claro del objetivo a cumplir para la correcta elección de métodos de aprendizaje congruentes y así, lograr optimizar el comportamiento de un agente en el entorno, donde los principales tipos de métodos de RL son: el RL basado en modelo (\textit{Model-based RL}, MBRL), RL sin modelo (\textit{Model-free RL}, MFRL) y el RL profundo (\textit{Deep RL}). El MBRL usa un modelo del entorno mediante el cual se aplican iteraciones de políticas o valores para el proceso, lo cual representa un enfoque más dirigido a la prueba y error en el entrenamiento del modelo con programación dinámica.  El MFRL efectua una relación más directa con el ambiente a controlar, únicamente basándose en experiencias obtenidas por contextualización como el caso de la aplicación de la función $Q$ con el \textit{Q-learning} o SARSA. El Deep RL combina los métodos anteriores con redes neuronales profundas, lo que le permite representar y procesar datos más complejos (alta dimensión) y mejorar el rendimiento del agente sin necesidad de extraer características manualmente del entorno \textcolor{SkyBlue}{DataScience}

En términos generales, para realizar un diseño de un controlador con aprendizaje reforzado es necesario el conocimiento en ingeniería en electrónica, en particular el diseño y construcción de sistemas electrónicos, teoría de sistemas, sistemas digitales, sensores y actuadores, de manera que se pueda garantizar la implementación eficiente, precisa y confiable que integre, además, técnicas del aprendizaje automático \textcolor{SkyBlue}{Control} \textcolor{SkyBlue}{BBVA} \textcolor{SkyBlue}{VideoIA}.

Como contexto para este proyecto, se parte del trabajo de Brenes Alfaro \textcolor{SkyBlue}{TesisJorge}, quien propuso un sistema basado en redes neuronales, para emular el comportamiento de una planta de laboratorio, específicamente el Péndulo Amortiguado a Hélice (PAMH), comúnmente utilizado en el Laboratorio de Control Automático de la Escuela de Ingeniería Electrónica del ITCR. Así, se cuenta con una red neuronal que se comporta de manera similar a la versión física de la planta, considerando perturbaciones y otros factores que definen el comportamiento de la planta real \textcolor{SkyBlue}{PAMHinfo}, y que permite entonces ser usada en enfoques libres de modelo para el diseño de controladores, usando técnicas de aprendizaje reforzado, sin arriesgar la integridad de la planta real, y permitiendo su uso en tiempo de simulación acelerado.

En este punto, se cuenta con un modelo del comportamiento de la planta de laboratorio PAMH. Sin embargo, no se ha propuesto aun ningún método de control basado en aprendizaje reforzado \textcolor{SkyBlue}{TesisJorge}, donde los métodos más comunes son basados en modelos y sin modelo. El primero presenta iteraciones con políticas o valores programados dinámicamente, mientras que el segundo se basa en cálculos de optimización con gradiente o libres de él. Además, se denomina el apartado de aprendizaje reforzado profundo (\textit{Deep RL}) como una combinación de los métodos mencionados \textcolor{SkyBlue}{DataScience}.

De esta manera, la problemática planteada desde el punto de vista ingenieril apunta a una premisa enfocada al aprendizaje automático aplicado mediante el RL para el control automático.

\subsection{Síntesis del problema}

Se carece de un sistema de control automático, que por medio de técnicas de aprendizaje reforzado, permita manipular el comportamiento de una planta de control no lineal.



\section{Enfoque de la solución}

Ahora que se conoce la problemática y el entorno de este proyecto que persigue aplicar el aprendizaje automático al control de una planta no lineal, es necesario plantear algunas opciones que permitan resolver dicho problema, las cuales se ven direccionadas a los métodos y algoritmos de aprendizaje existentes.

Así, se proponen tres alternativas que permiten el control de la planta PAMH (Figura \ref{fig:PAMH}) con diferentes frentes de operación de aprendizaje reforzado. Este tipo de aprendizaje en general mantiene una estructura como la mostrada en la Figura \ref{fig:EstructuraAR}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.3\textwidth]{fig/new/PAMH.png}
    \caption{Planta de laboratorio PAMH \textcolor{SkyBlue}{TesisJorge}.}
    \label{fig:PAMH}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{fig/new/EstructuraAR.png}
    \caption{Modelo de aprendizaje reforzado \textcolor{SkyBlue}{FiguraEstructAR}.}
    \label{fig:EstructuraAR}
\end{figure}

Se realizó la respectiva valoración con una matriz de Pugh que expusiera los puntos a considerar para la elección de la alternativa.

\subsection{Solución 1}

Una red neuronal recurrente (\textit{Recurrent Neural Network}, RNN) presenta su mejor desempeño en el reconocimiento de la voz, donde las secuencias de datos son considerablemente grandes para obtener un entrenamiento eficiente de un modelo, ya que este recorre la trayectoria de los datos en el tiempo, donde cada punto representa un grado de optimización del modelo \textcolor{SkyBlue}{DataScience}.

\subsection{Solución 2} \label{SolucionSeleccionada}

Los métodos de aprendizaje reforzado profundo (\textit{Deep Reinforcement Learning}, DRL) presentan un aumento de la demanda y desarrollo en el área de control, de manera que permiten realizar cálculos complejos y representarlos de manera eficiente en espacios de estados de dimensiones altas, logrando un muy buen desempeño en tareas como el reconocimiento de imágenes \textcolor{SkyBlue}{DataScience}.

\subsection{Solución 3}

Métodos clásicos de aprendizaje reforzado como los cálculos basados en el gradiente o las iteraciones de políticas o valores pueden llegar a representar un camino claro para lograr comportamientos deseados en aplicaciones de control, donde se encuentran diferentes algoritmos que permiten optimizar el entrenamiento de los modelos al aplicar fórmulas específicas para cada método y comportamiento deseado \textcolor{SkyBlue}{DataScience}.


\subsection{Selección de la solución}

Como se observa, cada alternativa corresponde a un modelo de trabajo al aplicar el aprendizaje automático en el control de una planta de laboratorio PAMH, de manera que se considerarán aspectos en los que se evidencia la elección de la solución \ref{SolucionSeleccionada} como la más adecuada para el trabajo en cuestión, esto mediante las valoración en la matriz de Pugh del cuadro \ref{tab:MatrizPugh}.

\begin{table}[h]
\caption{Matriz de Pugh de las opciones de solución.}
\begin{tabular}{lcccc}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Criterios}}} &
  \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Peso}}} &
  \multicolumn{3}{c|}{\textbf{Alternativas}} \\ \cline{3-5} 
\multicolumn{1}{|c|}{} &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{l|}{\textbf{Solución 1}} &
  \multicolumn{1}{l|}{\textbf{Solución 2}} &
  \multicolumn{1}{l|}{\textbf{Solución 3}} \\ \hline
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Fiabilidad de control del PAMH\end{tabular}} &
  \multicolumn{1}{c|}{4,5} &
  \multicolumn{1}{c|}{0} &
  \multicolumn{1}{c|}{+1} &
  \multicolumn{1}{c|}{+1} \\ \hline
\multicolumn{1}{|l|}{Costo económico} &
  \multicolumn{1}{c|}{4} &
  \multicolumn{1}{c|}{0} &
  \multicolumn{1}{c|}{+1} &
  \multicolumn{1}{c|}{-1} \\ \hline
\multicolumn{1}{|l|}{Tiempo de desarrollo} &
  \multicolumn{1}{c|}{3,5} &
  \multicolumn{1}{c|}{0} &
  \multicolumn{1}{c|}{+1} &
  \multicolumn{1}{c|}{-1} \\ \hline
\multicolumn{1}{|l|}{Código existente} &
  \multicolumn{1}{c|}{3} &
  \multicolumn{1}{c|}{+1} &
  \multicolumn{1}{c|}{+1} &
  \multicolumn{1}{c|}{+1} \\ \hline
\multicolumn{1}{|l|}{Optimización} &
  \multicolumn{1}{c|}{2,5} &
  \multicolumn{1}{c|}{-1} &
  \multicolumn{1}{c|}{+1} &
  \multicolumn{1}{c|}{0} \\ \hline
\multicolumn{1}{|l|}{Tiempo de entrenamiento} &
  \multicolumn{1}{c|}{2} &
  \multicolumn{1}{c|}{-1} &
  \multicolumn{1}{c|}{+1} &
  \multicolumn{1}{c|}{0} \\ \hline
\multicolumn{1}{|l|}{Datos de entrenamiento} &
  \multicolumn{1}{c|}{1,5} &
  \multicolumn{1}{c|}{0} &
  \multicolumn{1}{c|}{0} &
  \multicolumn{1}{c|}{+1} \\ \hline
\multicolumn{1}{|l|}{Innovación} &
  \multicolumn{1}{c|}{1} &
  \multicolumn{1}{c|}{+1} &
  \multicolumn{1}{c|}{+1} &
  \multicolumn{1}{c|}{+1} \\ \hline
 &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} \\ \cline{1-1} \cline{3-5} 
\multicolumn{1}{|l|}{\textbf{Suma general}} &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{-0.5} &
  \multicolumn{1}{c|}{20.5} &
  \multicolumn{1}{c|}{2.5} \\ \cline{1-1} \cline{3-5} 
\multicolumn{1}{|l|}{\textbf{Ranking}} &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{3"o} &
  \multicolumn{1}{c|}{1"o} &
  \multicolumn{1}{c|}{2"o} \\ \cline{1-1} \cline{3-5} 
\end{tabular}
\label{tab:MatrizPugh}
\end{table}


Con base en la matriz de Pugh desarrollada en el cuadro \ref{tab:MatrizPugh}, se seleccionaron ocho variables que permitieron puntuar los criterios para cada posible solución. 

En primera, se cuenta con la fiabilidad del control del PAMH, eso debido a que algunos métodos no encajan muy bien con el enfoque del proyecto, por lo que es necesario reaccionar en primera instancia con los objetivos que suelen sumarse a cada solución, donde la solución 1 no suele relacionarse con el control de un sistema en específico \textcolor{SkyBlue}{DataScience}.

El costo económico va en función del tiempo de desarrollo, compuesto del entrenamiento y optimización del modelo en cuestión, por lo que el costo computacional y presencial a largos periodos de tiempo es significativo, todo esto frente al tiempo limitado disponible para la elaboración del trabajo final de graduación. 

Cada método presenta características complejas respecto a la implementación de los modelos de aprendizaje automático actuales, por lo que es de vital importancia disponer de referencias bibliográficas que permitan el acceso a códigos de prueba y así, realizar las modificaciones pertinentes, de manera que el constante desarrollo de métodos de aprendizaje automático cumple con este punto.

Dada la teoría y las características de cada solución presentada, la optimización de cada método equivale a diferentes grados de complejidad, donde la solución 1 requiere ajustes adicionales de la estructura para lograrlo, mientras que el caso de los métodos clásicos de RL y DRL permiten un ajuste más cercano a la experiencia y recompensa, en este caso resaltando la solución 2 por su enfoque directo al control de comportamientos \textcolor{SkyBlue}{DataScience}.

Respecto al tiempo de entrenamiento, la estructura secuencial de las RNN castiga especialmente este punto, además de la cantidad de datos de entrenamiento, mientras que el RL mejora este ámbito al aprovechar los recursos computacionales, especialmente el caso del DRL. Es así que los métodos clásicos de RL requieren menor cantidad de datos de entrenamiento pero mayor tiempo de iteración para optimizar, superado fácilmente por el DRL \textcolor{SkyBlue}{DataScience}.

Por último, al tratarse de un área de estudio en auge, constantemente se publican nuevos avances y métodos para cada tipo de modelo de aprendizaje automático, de manera que a nivel general cada solución significa innovación en sus estructuras.

Así y en suma, la solución con valor aceptable se trata de la número 2, donde el caso a utilizar se trata del aprendizaje reforzado profundo (DRL), por sus cualidades más enfocadas al problema en cuestión del proyecto.

En la Figura \ref{fig:Diagbloques} se muestra el diagrama de bloques de la solución propuesta, lo cual permite plantear un primer enfoque de la metodología a desarrollar.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\textwidth]{fig/new/Diagbloques.png}
    \caption{Diagrama de bloques del proceso simplificado de aprendizaje reforzado profundo.}
    \label{fig:Diagbloques}
\end{figure}

\begin{comment}
Bajo dicha modalidad de trabajo se espera el cumplimiento de algunas condiciones necesarias para un correcto entrenamiento y validación del modelo a elaborar, las cuales se muestran en el cuadro \ref{tab:condiciones}

\begin{table}[!h]
\caption{Conjunto de condiciones mínimas necesarias para un entrenamiento eficiente del modelo.}
\label{tab:condiciones}
\centering
\begin{tabular}{|c|c|c|}
\hline
\centering\cellcolor{tableheader}\textcolor{white}{\textbf{Parámetro}} & \centering\cellcolor{tableheader}\textcolor{white}{\textbf{Entrenamiento}} & \cellcolor{tableheader}\textcolor{white}{\textbf{Validación y prueba}} \\ \hline
\textit{Conjunto de datos} & 500 episodios          & 100 episodios                \\ \hline
\textit{Duración}          & 3 segundos             & 3 segundos                   \\ \hline
\end{tabular}
\end{table}
\end{comment}


\begin{comment}
\begin{table}[!h]
\caption{Conjunto de requisitos para la comunicación del controlador mediante DRL.}
\label{tab:requisitos}
\centering
\begin{tabular}{|c|p{11cm}|}
\hline
\centering\cellcolor{tableheader}\textcolor{white}{\textbf{Número}} & {\centering\cellcolor{tableheader}\textcolor{white}{\textbf{Requisito}}} \\ \hline
$1$ & Sistema de captura lee el estado y aplica la acción en tiempo continuo. \\ \hline
$2$ & Sistema de captura de datos capaz de usarse con planta real y planta simulada. \\ \hline
$3$ & Sistema de captura puede acoplarse al controlador con señal de entrada y salida. \\ \hline
\end{tabular}
\end{table}



\section{Objetivo General}

Diseñar un sistema de aprendizaje automático para el control del ángulo de una planta no lineal PAMH.

\textbf{Indicador:} Sistema capaz de alcanzar un error angular inferior al $10\%$ frente a un estímulo constante. 

\subsection{Objetivos específicos}

\begin{enumerate}
    \item Seleccionar un método de aprendizaje reforzado apto para el control no lineal.

\textbf{Indicador:} Métrica de matriz de Pugh sobre métodos preseleccionados de aprendizaje reforzado.

    \item Diseñar la estrategia de captura de datos necesarios para el entrenamiento del modelo de aprendizaje reforzado que controle el modelo imitador del prototipo de laboratorio.

\textbf{Indicador:} Cumplimiento de los requisitos tabulados en el cuadro 2.

    \item Implementar el modelo de aprendizaje reforzado para el control del ángulo y entrenamiento del PAMH.

\textbf{Indicador:} Métrica de recompensa acumulada durante el proceso de entrenamiento y sistema entrenado que logra controlar el ángulo de la planta PAMH emulada.

    \item Evaluar el modelo de aprendizaje automático utilizado.

\textbf{Indicador:} Evaluación de al menos 5 configuraciones distintas de hiperparámetros del modelo seleccionado.


\end{enumerate}





\end{comment}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
