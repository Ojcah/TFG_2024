{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from ddpg import DDPG\n",
    "from utils.noise import OrnsteinUhlenbeckActionNoise\n",
    "from utils.replay_memory import ReplayMemory, Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "\n",
    "    def action(self, action):\n",
    "        \"\"\"\n",
    "        Normalizes the actions to be in between action_space.high and action_space.low.\n",
    "        If action_space.low == -action_space.high, this is equals to action_space.high*action.\n",
    "\n",
    "        :param action:\n",
    "        :return: normalized action\n",
    "        \"\"\"\n",
    "        action = (action + 1) / 2  # [-1, 1] => [0, 1]\n",
    "        action *= (self.action_space.high - self.action_space.low)\n",
    "        action += self.action_space.low\n",
    "        return action\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        \"\"\"\n",
    "        Reverts the normalization\n",
    "\n",
    "        :param action:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        action -= self.action_space.low\n",
    "        action /= (self.action_space.high - self.action_space.low)\n",
    "        action = action * 2 - 1\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--env ENV] [--target_angle TARGET_ANGLE]\n",
      "                             [--render_train RENDER_TRAIN]\n",
      "                             [--render_eval RENDER_EVAL]\n",
      "                             [--load_model LOAD_MODEL] [--save_dir SAVE_DIR]\n",
      "                             [--seed SEED] [--timesteps TIMESTEPS]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--replay_size REPLAY_SIZE] [--gamma GAMMA]\n",
      "                             [--tau TAU] [--noise_stddev NOISE_STDDEV]\n",
      "                             [--hidden_size HIDDEN_SIZE HIDDEN_SIZE]\n",
      "                             [--n_test_cycles N_TEST_CYCLES]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\Kley2\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-22336HWSz5qrTivdx.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kley2\\anaconda3\\envs\\forPPO\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Create logger\n",
    "logger = logging.getLogger('train')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Libdom raises an error if this is not set to true on Mac OSX\n",
    "# see https://github.com/openai/spinningup/issues/16 for more information\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "# Parse given arguments\n",
    "# gamma, tau, hidden_size, replay_size, batch_size, hidden_size are taken from the original paper\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--env\", default=\"Pendulum-v1\", help=\"the environment on which the agent should be trained \"\n",
    "                         \"(Default: Pendulum-v1\")\n",
    "parser.add_argument(\"--target_angle\", default=0, type=int,\n",
    "                    help=\"Target angle to control (default: 0 degrees)\")\n",
    "parser.add_argument(\"--render_train\", default=False, type=bool,\n",
    "                    help=\"Render the training steps (default: False)\")\n",
    "parser.add_argument(\"--render_eval\", default=False, type=bool,\n",
    "                    help=\"Render the evaluation steps (default: False)\")\n",
    "parser.add_argument(\"--load_model\", default=False, type=bool,\n",
    "                    help=\"Load a pretrained model (default: False)\")\n",
    "parser.add_argument(\"--save_dir\", default=\"./models/\",\n",
    "                    help=\"Dir. path to save and load a model (default: ./models/)\")\n",
    "parser.add_argument(\"--seed\", default=0, type=int,\n",
    "                    help=\"Random seed (default: 0)\")\n",
    "parser.add_argument(\"--timesteps\", default=1e6, type=int,\n",
    "                    help=\"Num. of total timesteps of training (default: 1e6)\")\n",
    "parser.add_argument(\"--batch_size\", default=64, type=int,\n",
    "                    help=\"Batch size (default: 64; OpenAI: 128)\")\n",
    "parser.add_argument(\"--replay_size\", default=1e6, type=int,\n",
    "                    help=\"Size of the replay buffer (default: 1e6; OpenAI: 1e5)\")\n",
    "parser.add_argument(\"--gamma\", default=0.99,\n",
    "                    help=\"Discount factor (default: 0.99)\")\n",
    "parser.add_argument(\"--tau\", default=0.001,\n",
    "                    help=\"Update factor for the soft update of the target networks (default: 0.001)\")\n",
    "parser.add_argument(\"--noise_stddev\", default=0.2, type=int,\n",
    "                    help=\"Standard deviation of the OU-Noise (default: 0.2)\")\n",
    "parser.add_argument(\"--hidden_size\", nargs=2, default=[400, 300], type=tuple,\n",
    "                    help=\"Num. of units of the hidden layers (default: [400, 300]; OpenAI: [64, 64])\")\n",
    "parser.add_argument(\"--n_test_cycles\", default=10, type=int,\n",
    "                    help=\"Num. of episodes in the evaluation phases (default: 10; OpenAI: 20)\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(\"Using {}\".format(device))\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    env=\"Pendulum-v1\",\n",
    "    target_angle=0, \n",
    "    render_train=True, \n",
    "    render_eval=False, \n",
    "    load_model=False, \n",
    "    save_dir=\"./models/\", \n",
    "    seed=0, \n",
    "    timesteps=1_000, \n",
    "    batch_size=64, \n",
    "    replay_size=1_000_000, \n",
    "    gamma=0.99, \n",
    "    tau=0.001, \n",
    "    noise_stddev= 0.2, \n",
    "    hidden_size=[400, 300], \n",
    "    n_test_cycles=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(observ, torque, target_angle): # Todos los valores estan en radianes\n",
    "\t\ttheta = math.atan2(observ[1],observ[0])\n",
    "\t\ttheta_dot = observ[2]\n",
    "\t\t\n",
    "\t\ttheta_n = ((theta + np.pi) % (2*np.pi)) - np.pi\n",
    "\n",
    "\t\ttheta_error = np.abs(theta_n - target_angle)\n",
    "\t\t\n",
    "\t\t#torque_castigo = (torque**2) - np.minimum(2-np.absolute(torque),0)\n",
    "\t\ttorque_castigo = 0.001 * (torque**2)\n",
    "\t\tcosts = (theta_error**2) + 0.1 * (theta_dot**2) + torque_castigo\n",
    "\t\tif theta_error <= 0.087: # ~ 5Â°\n",
    "\t\t\treward_n = -costs + math.exp(-(8*theta_error)**2)\n",
    "\t\telse:\n",
    "\t\t\treward_n = -costs\n",
    "\t\t# reward_n = -costs\n",
    "\t\treturn reward_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    target_angle = args.target_angle\n",
    "\n",
    "    # Define the directory where to save and load models\n",
    "    checkpoint_dir = args.save_dir + args.env\n",
    "    writer = SummaryWriter('runs/run_1')\n",
    "\n",
    "    # Create the env\n",
    "    kwargs = dict()\n",
    "    if args.env == 'RoboschoolInvertedPendulumSwingup-v1':\n",
    "        # 'swingup=True' must be passed as an argument\n",
    "        # See pull request 'https://github.com/openai/roboschool/pull/192'\n",
    "        kwargs['swingup'] = True\n",
    "    env = gym.make(args.env, **kwargs)\n",
    "    env = NormalizedActions(env)\n",
    "\n",
    "    # Define the reward threshold when the task is solved (if existing) for model saving\n",
    "    reward_threshold = gym.spec(args.env).reward_threshold if gym.spec(\n",
    "        args.env).reward_threshold is not None else np.inf\n",
    "\n",
    "    # Set random seed for all used libraries where possible\n",
    "    env.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Define and build DDPG agent\n",
    "    hidden_size = tuple(args.hidden_size)\n",
    "    agent = DDPG(args.gamma,\n",
    "                 args.tau,\n",
    "                 hidden_size,\n",
    "                 env.observation_space.shape[0],\n",
    "                 env.action_space,\n",
    "                 checkpoint_dir=checkpoint_dir\n",
    "                 )\n",
    "\n",
    "    # Initialize replay memory\n",
    "    memory = ReplayMemory(int(args.replay_size))\n",
    "\n",
    "    # Initialize OU-Noise\n",
    "    nb_actions = env.action_space.shape[-1]\n",
    "    ou_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(nb_actions),\n",
    "                                            sigma=float(args.noise_stddev) * np.ones(nb_actions))\n",
    "\n",
    "    # Define counters and other variables\n",
    "    start_step = 0\n",
    "    # timestep = start_step\n",
    "    if args.load_model:\n",
    "        # Load agent if necessary\n",
    "        start_step, memory = agent.load_checkpoint()\n",
    "    timestep = start_step // 10000 + 1\n",
    "    rewards, policy_losses, value_losses, mean_test_rewards = [], [], [], []\n",
    "    epoch = 0\n",
    "    t = 0\n",
    "    time_last_checkpoint = time.time()\n",
    "\n",
    "    # Start training\n",
    "    logger.info('Train agent on {} env'.format({env.unwrapped.spec.id}))\n",
    "    logger.info('Doing {} timesteps'.format(args.timesteps))\n",
    "    logger.info('Start at timestep {0} with t = {1}'.format(timestep, t))\n",
    "    logger.info('Start training at {}'.format(time.strftime('%a, %d %b %Y %H:%M:%S GMT', time.localtime())))\n",
    "\n",
    "    while timestep <= args.timesteps:\n",
    "        ou_noise.reset()\n",
    "        epoch_return = 0\n",
    "\n",
    "        state = torch.Tensor([env.reset()]).to(device)\n",
    "        while True:\n",
    "            if args.render_train:\n",
    "                env.render()\n",
    "\n",
    "            action = agent.calc_action(state, ou_noise)\n",
    "            #next_state, reward, done, _ = env.step(action.cpu().numpy()[0])\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.cpu().numpy()[0])\n",
    "            done = terminated or truncated\n",
    "            timestep += 1\n",
    "\n",
    "            reward = calculate_reward(next_state, action.item(), target_angle)\n",
    "\n",
    "            epoch_return += reward\n",
    "\n",
    "            mask = torch.Tensor([done]).to(device)\n",
    "            reward = torch.Tensor([reward]).to(device)\n",
    "            next_state = torch.Tensor([next_state]).to(device)\n",
    "\n",
    "            memory.push(state, action, mask, next_state, reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            epoch_value_loss = 0\n",
    "            epoch_policy_loss = 0\n",
    "\n",
    "            if len(memory) > args.batch_size:\n",
    "                transitions = memory.sample(args.batch_size)\n",
    "                # Transpose the batch\n",
    "                # (see http://stackoverflow.com/a/19343/3343043 for detailed explanation).\n",
    "                batch = Transition(*zip(*transitions))\n",
    "\n",
    "                # Update actor and critic according to the batch\n",
    "                value_loss, policy_loss = agent.update_params(batch)\n",
    "\n",
    "                epoch_value_loss += value_loss\n",
    "                epoch_policy_loss += policy_loss\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(epoch_return)\n",
    "        value_losses.append(epoch_value_loss)\n",
    "        policy_losses.append(epoch_policy_loss)\n",
    "        writer.add_scalar('epoch/return', epoch_return, epoch)\n",
    "\n",
    "        # Test every 10th episode (== 1e4) steps for a number of test_epochs epochs\n",
    "        if timestep >= 10000 * t:\n",
    "            t += 1\n",
    "            test_rewards = []\n",
    "            for _ in range(args.n_test_cycles):\n",
    "                state = torch.Tensor([env.reset()]).to(device)\n",
    "                test_reward = 0\n",
    "                while True:\n",
    "                    if args.render_eval:\n",
    "                        env.render()\n",
    "\n",
    "                    action = agent.calc_action(state)  # Selection without noise\n",
    "\n",
    "                    #next_state, reward, done, _ = env.step(action.cpu().numpy()[0])\n",
    "                    next_state, reward, terminated, truncated, _ = env.step(action.cpu().numpy()[0])\n",
    "                    done = terminated or truncated\n",
    "\n",
    "                    reward = calculate_reward(next_state, action.item(), target_angle)\n",
    "\n",
    "                    test_reward += reward\n",
    "\n",
    "                    next_state = torch.Tensor([next_state]).to(device)\n",
    "\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "                test_rewards.append(test_reward)\n",
    "\n",
    "            mean_test_rewards.append(np.mean(test_rewards))\n",
    "\n",
    "            for name, param in agent.actor.named_parameters():\n",
    "                writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
    "            for name, param in agent.critic.named_parameters():\n",
    "                writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
    "\n",
    "            writer.add_scalar('test/mean_test_return', mean_test_rewards[-1], epoch)\n",
    "            logger.info(\"Epoch: {}, current timestep: {}, last reward: {}, \"\n",
    "                        \"mean reward: {}, mean test reward {}\".format(epoch,\n",
    "                                                                      timestep,\n",
    "                                                                      rewards[-1],\n",
    "                                                                      np.mean(rewards[-10:]),\n",
    "                                                                      np.mean(test_rewards)))\n",
    "\n",
    "            # Save if the mean of the last three averaged rewards while testing\n",
    "            # is greater than the specified reward threshold\n",
    "            # TODO: Option if no reward threshold is given\n",
    "            if np.mean(mean_test_rewards[-3:]) >= reward_threshold:\n",
    "                agent.save_checkpoint(timestep, memory)\n",
    "                time_last_checkpoint = time.time()\n",
    "                logger.info('Saved model at {}'.format(time.strftime('%a, %d %b %Y %H:%M:%S GMT', time.localtime())))\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    agent.save_checkpoint(timestep, memory)\n",
    "    logger.info('Saved model at endtime {}'.format(time.strftime('%a, %d %b %Y %H:%M:%S GMT', time.localtime())))\n",
    "    logger.info('Stopping training at {}'.format(time.strftime('%a, %d %b %Y %H:%M:%S GMT', time.localtime())))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forPPO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
