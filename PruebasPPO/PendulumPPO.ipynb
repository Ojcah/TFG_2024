{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode=\"human\", g=9.81)\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "# observation, info = env.reset()\n",
    "\n",
    "# for _ in range(250):\n",
    "#     action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "#     observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "#     print(observation, reward, terminated, truncated, info)\n",
    "\n",
    "#     if terminated or truncated:\n",
    "#         observation, info = env.reset()\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self,n_observations, n_actions):\n",
    "        super(Agent, self).__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(n_observations, 128),\n",
    "            nn.Tanh(),    ##\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh(),    ##\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            nn.Linear(n_observations, 128),\n",
    "            nn.Tanh(),    ##\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Tanh(),    ##\n",
    "            nn.Linear(128, n_actions),\n",
    "        )\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        action_mean = self.actor_mean(x)\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99        # discount factor as mentioned in the previous section\n",
    "EPS_START = 0.9     # starting value of epsilon\n",
    "EPS_END = 0.05      # final value of epsilon\n",
    "EPS_DECAY = 1000    # controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "TAU = 0.005         # update rate of the target network\n",
    "LR = 1e-4           # learning rate of the ``AdamW`` optimizer\n",
    "\n",
    "total_timesteps = 2000000\n",
    "\n",
    "num_steps = 128\n",
    "num_envs = 4\n",
    "\n",
    "batch_size = int(num_envs * num_steps)\n",
    "\n",
    "n_actions = np.size(env.action_space)\n",
    "\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "agent = Agent(n_observations, n_actions).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(agent.parameters(), lr=LR, eps=1e-5)\n",
    "\n",
    "obs = torch.zeros((num_steps, num_envs) + n_observations).to(device)\n",
    "actions = torch.zeros((num_steps, num_envs) + n_actions).to(device)\n",
    "logprops = torch.zeros((num_steps, num_envs)).to(device)\n",
    "rewards = torch.zeros((num_steps, num_envs)).to(device)\n",
    "dones = torch.zeros((num_steps, num_envs)).to(device)\n",
    "values = torch.zeros((num_steps, num_envs)).to(device)\n",
    "\n",
    "global_steps = 0\n",
    "start_time = time.time()\n",
    "next_obs = torch.Tensor(state).to(device)\n",
    "next_done = torch.zeros(num_envs).to(device)\n",
    "num_updates = total_timesteps//batch_size\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forDQN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
