{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from ppo import PPO\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\tThis file contains the arguments to parse at command line.\n",
    "\tFile main.py will call get_args, which then the arguments\n",
    "\twill be returned.\n",
    "\"\"\"\n",
    "def get_args():\n",
    "\t\"\"\"\n",
    "\t\tDescription:\n",
    "\t\tParses arguments at command line.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tNone\n",
    "\n",
    "\t\tReturn:\n",
    "\t\t\targs - the arguments parsed\n",
    "\t\"\"\"\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\n",
    "\tparser.add_argument('--mode', dest='mode', type=str, default='train')              # can be 'train' or 'test'\n",
    "\tparser.add_argument('--target_angle', dest='target_angle', type=str, default='0')   # your critic model filename\n",
    "\tparser.add_argument('--actor_model', dest='actor_model', type=str, default='')     # your actor model filename\n",
    "\tparser.add_argument('--critic_model', dest='critic_model', type=str, default='')   # your critic model filename\n",
    "\t\n",
    "\n",
    "\targs = parser.parse_args()\n",
    "\n",
    "\treturn args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\tThis file contains a neural network module for us to\n",
    "\tdefine our actor and critic networks in PPO.\n",
    "\"\"\"\n",
    "class FeedForwardNN(nn.Module):\n",
    "\t\"\"\"\n",
    "\t\tA standard in_dim-64-64-out_dim Feed Forward Neural Network.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, in_dim, out_dim):\n",
    "\t\t\"\"\"\n",
    "\t\t\tInitialize the network and set up the layers.\n",
    "\n",
    "\t\t\tParameters:\n",
    "\t\t\t\tin_dim - input dimensions as an int\n",
    "\t\t\t\tout_dim - output dimensions as an int\n",
    "\n",
    "\t\t\tReturn:\n",
    "\t\t\t\tNone\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(FeedForwardNN, self).__init__()\n",
    "\n",
    "\t\tself.layer1 = nn.Linear(in_dim, 64)\n",
    "\t\tself.layer2 = nn.Linear(64, 64)\n",
    "\t\tself.layer3 = nn.Linear(64, out_dim)\n",
    "\n",
    "\tdef forward(self, obs):\n",
    "\t\t\"\"\"\n",
    "\t\t\tRuns a forward pass on the neural network.\n",
    "\n",
    "\t\t\tParameters:\n",
    "\t\t\t\tobs - observation to pass as input\n",
    "\n",
    "\t\t\tReturn:\n",
    "\t\t\t\toutput - the output of our forward pass\n",
    "\t\t\"\"\"\n",
    "\t\t# Convert observation to tensor if it's a numpy array\n",
    "\t\tif isinstance(obs, np.ndarray):\n",
    "\t\t\tobs = torch.tensor(obs, dtype=torch.float, device=device)\n",
    "\n",
    "\t\tactivation1 = F.relu(self.layer1(obs))\n",
    "\t\tactivation2 = F.relu(self.layer2(activation1))\n",
    "\t\toutput = self.layer3(activation2)\n",
    "\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\tThis file is used only to evaluate our trained policy/actor after\n",
    "\ttraining in main.py with ppo.py. I wrote this file to demonstrate\n",
    "\tthat our trained policy exists independently of our learning algorithm,\n",
    "\twhich resides in ppo.py. Thus, we can test our trained policy without \n",
    "\trelying on ppo.py.\n",
    "\"\"\"\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n",
    "\n",
    "ep_lens = []\n",
    "ep_rets = []\n",
    "\n",
    "def calculate_reward(observ, torque, target_angle): # Todos los valores estan en radianes\n",
    "\t\ttheta = math.atan2(observ[1],observ[0])\n",
    "\t\ttheta_dot = observ[2]\n",
    "\t\t\n",
    "\t\ttheta_n = ((theta + np.pi) % (2*np.pi)) - np.pi\n",
    "\n",
    "\t\ttheta_error = np.abs(theta_n - target_angle)\n",
    "\t\t\n",
    "\t\t#torque_castigo = (torque**2) - np.minimum(2-np.absolute(torque),0)\n",
    "\t\ttorque_castigo = 0.001 * (torque**2)\n",
    "\t\tcosts = (theta_error**2) + 0.1 * (theta_dot**2) + torque_castigo\n",
    "\t\tif theta_error <= 0.087: # ~ 5Â°\n",
    "\t\t\treward_n = -costs + math.exp(-(8*theta_error)**2)\n",
    "\t\telse:\n",
    "\t\t\treward_n = -costs\n",
    "\t\t# reward_n = -costs\n",
    "\t\treturn reward_n.item()\n",
    "\n",
    "def _log_summary(ep_len, ep_ret, ep_num):\n",
    "\t\t\"\"\"\n",
    "\t\t\tPrint to stdout what we've logged so far in the most recent episode.\n",
    "\n",
    "\t\t\tParameters:\n",
    "\t\t\t\tNone\n",
    "\n",
    "\t\t\tReturn:\n",
    "\t\t\t\tNone\n",
    "\t\t\"\"\"\n",
    "\t\t# Round decimal places for more aesthetic logging messages\n",
    "\t\tep_len = str(round(ep_len, 2))\n",
    "\t\tep_ret = str(round(ep_ret, 2))\n",
    "\n",
    "\t\t# Print logging statements\n",
    "\t\tprint(flush=True)\n",
    "\t\tprint(f\"-------------------- Episode #{ep_num} --------------------\", flush=True)\n",
    "\t\tprint(f\"Episodic Length: {ep_len}\", flush=True)\n",
    "\t\tprint(f\"Episodic Return: {ep_ret}\", flush=True)\n",
    "\t\tprint(f\"------------------------------------------------------\", flush=True)\n",
    "\t\tprint(flush=True)\n",
    "  \n",
    "\t\t\n",
    "\t\t# plt.figure(1)\n",
    "\t\t# # Round decimal places for more aesthetic logging messages\n",
    "\t\t# ep_len = round(ep_len, 2)\n",
    "\t\t# ep_ret = round(ep_ret, 2)\n",
    "\n",
    "\t\t# # Append to lists\n",
    "\t\t# if ep_lens:\n",
    "\t\t# \tep_lens.append(ep_lens[-1] + ep_len)\n",
    "\t\t# else:\n",
    "\t\t# \tep_lens.append(ep_len)\n",
    "\t\t# ep_rets.append(ep_ret)\n",
    "\n",
    "\t\t# # Clear previous plot\n",
    "\t\t# plt.clf()\n",
    "\n",
    "\t\t# # Plot\n",
    "\t\t# plt.plot(ep_lens, ep_rets, color='blue')\n",
    "\t\t# plt.title(f'Episode #{ep_num} - Episodic Length vs Episodic Return')\n",
    "\t\t# plt.xlabel('Episodic Length')\n",
    "\t\t# plt.ylabel('Episodic Return')\n",
    "\t\t# plt.grid(True)\n",
    "\t\t# plt.pause(0.01)\n",
    "\t\t# if is_ipython:\n",
    "\t\t# \tdisplay.display(plt.gcf())\n",
    "\t\t# \tdisplay.clear_output(wait=True)\n",
    "\n",
    "\n",
    "def rollout(policy, env, render, target_angle):\n",
    "\t\"\"\"\n",
    "\t\tReturns a generator to roll out each episode given a trained policy and\n",
    "\t\tenvironment to test on. \n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tpolicy - The trained policy to test\n",
    "\t\t\tenv - The environment to evaluate the policy on\n",
    "\t\t\trender - Specifies whether to render or not\n",
    "\t\t\n",
    "\t\tReturn:\n",
    "\t\t\tA generator object rollout, or iterable, which will return the latest\n",
    "\t\t\tepisodic length and return on each iteration of the generator.\n",
    "\n",
    "\t\tNote:\n",
    "\t\t\tIf you're unfamiliar with Python generators, check this out:\n",
    "\t\t\t\thttps://wiki.python.org/moin/Generators\n",
    "\t\t\tIf you're unfamiliar with Python \"yield\", check this out:\n",
    "\t\t\t\thttps://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n",
    "\t\"\"\"\n",
    "\t# Rollout until user kills process\n",
    "\twhile True:\n",
    "\t\t# obs = env.reset()\t\t\t\t# For Gym version\n",
    "\t\tobs, _ = env.reset()\t\t\t# for Gymnasium version\n",
    "\t\tdone = False\n",
    "\n",
    "\t\t# number of timesteps so far\n",
    "\t\tt = 0\n",
    "\n",
    "\t\t# Logging data\n",
    "\t\tep_len = 0            # episodic length\n",
    "\t\tep_ret = 0            # episodic return\n",
    "\n",
    "\t\twhile not done:\n",
    "\t\t\tt += 1\n",
    "\n",
    "\t\t\t# Render environment if specified, off by default\n",
    "\t\t\tif render:\n",
    "\t\t\t\tenv.render()\n",
    "\n",
    "\t\t\t# Query deterministic action from policy and run it\n",
    "\t\t\taction = policy(obs).detach().cpu().numpy()\n",
    "\t\t\t#obs, rew, done, _ = env.step(action)\t\t\t\t\t# For Gym version\n",
    "\t\t\tobs, rew, terminated, truncated, _ = env.step(action)\t# For Gymnasium version\n",
    "\n",
    "\t\t\tdone = terminated or truncated\t\t\t\t\t\t\t# For Gymnasium\n",
    "\n",
    "\t\t\trew = calculate_reward(obs, action, math.radians(target_angle))\n",
    "\n",
    "\t\t\t# Sum all episodic rewards as we go along\n",
    "\t\t\tep_ret += rew\n",
    "\t\t\t\n",
    "\t\t# Track episodic length\n",
    "\t\tep_len = t\n",
    "\t\t\n",
    "\t\t# returns episodic length and return in this iteration\n",
    "\t\tyield ep_len, ep_ret\n",
    "\n",
    "def eval_policy(policy, env, render=False, target_angle=0):\n",
    "\t\"\"\"\n",
    "\t\tThe main function to evaluate our policy with. It will iterate a generator object\n",
    "\t\t\"rollout\", which will simulate each episode and return the most recent episode's\n",
    "\t\tlength and return. We can then log it right after. And yes, eval_policy will run\n",
    "\t\tforever until you kill the process. \n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tpolicy - The trained policy to test, basically another name for our actor model\n",
    "\t\t\tenv - The environment to test the policy on\n",
    "\t\t\trender - Whether we should render our episodes. False by default.\n",
    "\n",
    "\t\tReturn:\n",
    "\t\t\tNone\n",
    "\n",
    "\t\tNOTE: To learn more about generators, look at rollout's function description\n",
    "\t\"\"\"\n",
    "\t# Rollout with the policy and environment, and log each episode's data\n",
    "\tfor ep_num, (ep_len, ep_ret) in enumerate(rollout(policy, env, render, target_angle)):\n",
    "\t\t_log_summary(ep_len=ep_len, ep_ret=ep_ret, ep_num=ep_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, hyperparameters, target_angle, actor_model, critic_model):\n",
    "\t\"\"\"\n",
    "\t\tTrains the model.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tenv - the environment to train on\n",
    "\t\t\thyperparameters - a dict of hyperparameters to use, defined in main\n",
    "\t\t\tactor_model - the actor model to load in if we want to continue training\n",
    "\t\t\tcritic_model - the critic model to load in if we want to continue training\n",
    "\n",
    "\t\tReturn:\n",
    "\t\t\tNone\n",
    "\t\"\"\"\t\n",
    "\tprint(f\"Training\", flush=True)\n",
    "\n",
    "\t# Create a model for PPO.\n",
    "\tmodel = PPO(policy_class=FeedForwardNN, env=env, **hyperparameters)\n",
    "\n",
    "\t# Tries to load in an existing actor/critic model to continue training on\n",
    "\tif actor_model != '' and critic_model != '':\n",
    "\t\tprint(f\"Loading in {actor_model} and {critic_model}...\", flush=True)\n",
    "\t\tmodel.actor.load_state_dict(torch.load(actor_model))\n",
    "\t\tmodel.critic.load_state_dict(torch.load(critic_model))\n",
    "\t\tprint(f\"Successfully loaded.\", flush=True)\n",
    "\telif actor_model != '' or critic_model != '': # Don't train from scratch if user accidentally forgets actor/critic model\n",
    "\t\tprint(f\"Error: Either specify both actor/critic models or none at all. We don't want to accidentally override anything!\")\n",
    "\t\tsys.exit(0)\n",
    "\telse:\n",
    "\t\tprint(f\"Training from scratch.\", flush=True)\n",
    "\n",
    "\t# Train the PPO model with a specified total timesteps\n",
    "\t# NOTE: You can change the total timesteps here, I put a big number just because\n",
    "\t# you can kill the process whenever you feel like PPO is converging\n",
    "\t#model.learn(total_timesteps=200_000_000, target_angle=target_angle)\n",
    "\t#model.learn(total_timesteps=1_000_000, target_angle=target_angle)\n",
    "\tmodel.learn(total_timesteps=100_000, target_angle=target_angle)\n",
    "\n",
    "def test(env, target_angle, actor_model):\n",
    "\t\"\"\"\n",
    "\t\tTests the model.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tenv - the environment to test the policy on\n",
    "\t\t\tactor_model - the actor model to load in\n",
    "\n",
    "\t\tReturn:\n",
    "\t\t\tNone\n",
    "\t\"\"\"\n",
    "\tprint(f\"Testing {actor_model}\", flush=True)\n",
    "\n",
    "\t# If the actor model is not specified, then exit\n",
    "\tif actor_model == '':\n",
    "\t\tprint(f\"Didn't specify model file. Exiting.\", flush=True)\n",
    "\t\tsys.exit(0)\n",
    "\n",
    "\t# Extract out dimensions of observation and action spaces\n",
    "\tobs_dim = env.observation_space.shape[0]\n",
    "\tact_dim = env.action_space.shape[0]\n",
    "\n",
    "\t# Build our policy the same way we build our actor model in PPO\n",
    "\tpolicy = FeedForwardNN(obs_dim, act_dim).to(device)\n",
    "\n",
    "\t# Load in the actor model saved by the PPO algorithm\n",
    "\tpolicy.load_state_dict(torch.load(actor_model))\n",
    "\n",
    "\t# Evaluate our policy with a separate module, eval_policy, to demonstrate\n",
    "\t# that once we are done training the model/policy with ppo.py, we no longer need\n",
    "\t# ppo.py since it only contains the training algorithm. The model/policy itself exists\n",
    "\t# independently as a binary file that can be loaded in with torch.\n",
    "\teval_policy(policy=policy, env=env, render=True, target_angle=target_angle)\n",
    "\n",
    "def main(args):\n",
    "\t\"\"\"\n",
    "\t\tThe main function to run.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\targs - the arguments parsed from command line\n",
    "\n",
    "\t\tReturn:\n",
    "\t\t\tNone\n",
    "\t\"\"\"\n",
    "\t# NOTE: Here's where you can set hyperparameters for PPO. I don't include them as part of\n",
    "\t# ArgumentParser because it's too annoying to type them every time at command line. Instead, you can change them here.\n",
    "\t# To see a list of hyperparameters, look in ppo.py at function _init_hyperparameters\n",
    "\thyperparameters = {\n",
    "\t\t\t\t'timesteps_per_batch': 2048, \n",
    "\t\t\t\t'max_timesteps_per_episode': 200, \n",
    "\t\t\t\t'gamma': 0.99, \n",
    "\t\t\t\t'n_updates_per_iteration': 10,\n",
    "\t\t\t\t'lr': 3e-4, \n",
    "\t\t\t\t'clip': 0.2,\n",
    "\t\t\t\t'render': True,\n",
    "\t\t\t\t'render_every_i': 10\n",
    "\t\t\t  }\n",
    "\n",
    "\t# Creates the environment we'll be running. If you want to replace with your own\n",
    "\t# custom environment, note that it must inherit Gym and have both continuous\n",
    "\t# observation and action spaces.\n",
    "\t#env = gym.make('Pendulum-v1', render_mode=\"human\", g=9.81)\n",
    "\tenv = gym.make('Pendulum-v1', render_mode=\"rgb_array\", g=9.81)\n",
    "\n",
    "\t# Train or test, depending on the mode specified\n",
    "\tif args.mode == 'train':\n",
    "\t\ttrain(env=env, hyperparameters=hyperparameters, target_angle=int(args.target_angle), actor_model=args.actor_model, critic_model=args.critic_model)\n",
    "\telse:\n",
    "\t\ttest(env=env, target_angle=int(args.target_angle), actor_model=args.actor_model)\n",
    "\tenv.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t#args = get_args() # Parse arguments from command line # Just for .py files\n",
    "\targs = argparse.Namespace(mode='train', target_angle='135', actor_model='', critic_model='')\n",
    "\t#args = argparse.Namespace(mode='test', target_angle='135', actor_model='actor-critic/ppo_actor.pth', critic_model='actor-critic/ppo_critic.pth')\n",
    "\tmain(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba = np.array([-2.45])\n",
    "\n",
    "print(type(prueba))\n",
    "\n",
    "print(prueba.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forPPO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
