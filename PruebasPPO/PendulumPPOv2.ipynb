{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from ppo import PPO\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\tThis file contains the arguments to parse at command line.\n",
    "\tFile main.py will call get_args, which then the arguments\n",
    "\twill be returned.\n",
    "\"\"\"\n",
    "def get_args():\n",
    "\t\"\"\"\n",
    "\t\tDescription:\n",
    "\t\tParses arguments at command line.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tNone\n",
    "\n",
    "\t\tReturn:\n",
    "\t\t\targs - the arguments parsed\n",
    "\t\"\"\"\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\n",
    "\tparser.add_argument('--mode', dest='mode', type=str, default='train')              # can be 'train' or 'test'\n",
    "\tparser.add_argument('--target_angle', dest='target_angle', type=str, default='0')   # your critic model filename\n",
    "\tparser.add_argument('--actor_model', dest='actor_model', type=str, default='')     # your actor model filename\n",
    "\tparser.add_argument('--critic_model', dest='critic_model', type=str, default='')   # your critic model filename\n",
    "\t\n",
    "\n",
    "\targs = parser.parse_args()\n",
    "\n",
    "\treturn args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\tThis file contains a neural network module for us to\n",
    "\tdefine our actor and critic networks in PPO.\n",
    "\"\"\"\n",
    "class FeedForwardNN(nn.Module):\n",
    "\t\"\"\"\n",
    "\t\tA standard in_dim-64-64-out_dim Feed Forward Neural Network.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, in_dim, out_dim):\n",
    "\t\t\"\"\"\n",
    "\t\t\tInitialize the network and set up the layers.\n",
    "\n",
    "\t\t\tParameters:\n",
    "\t\t\t\tin_dim - input dimensions as an int\n",
    "\t\t\t\tout_dim - output dimensions as an int\n",
    "\n",
    "\t\t\tReturn:\n",
    "\t\t\t\tNone\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(FeedForwardNN, self).__init__()\n",
    "\n",
    "\t\tself.layer1 = nn.Linear(in_dim, 64)\n",
    "\t\tself.layer2 = nn.Linear(64, 64)\n",
    "\t\tself.layer3 = nn.Linear(64, out_dim)\n",
    "\n",
    "\tdef forward(self, obs):\n",
    "\t\t\"\"\"\n",
    "\t\t\tRuns a forward pass on the neural network.\n",
    "\n",
    "\t\t\tParameters:\n",
    "\t\t\t\tobs - observation to pass as input\n",
    "\n",
    "\t\t\tReturn:\n",
    "\t\t\t\toutput - the output of our forward pass\n",
    "\t\t\"\"\"\n",
    "\t\t# Convert observation to tensor if it's a numpy array\n",
    "\t\tif isinstance(obs, np.ndarray):\n",
    "\t\t\tobs = torch.tensor(obs, dtype=torch.float, device=device)\n",
    "\n",
    "\t\tactivation1 = F.relu(self.layer1(obs))\n",
    "\t\tactivation2 = F.relu(self.layer2(activation1))\n",
    "\t\toutput = self.layer3(activation2)\n",
    "\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\tThis file is used only to evaluate our trained policy/actor after\n",
    "\ttraining in main.py with ppo.py. I wrote this file to demonstrate\n",
    "\tthat our trained policy exists independently of our learning algorithm,\n",
    "\twhich resides in ppo.py. Thus, we can test our trained policy without \n",
    "\trelying on ppo.py.\n",
    "\"\"\"\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n",
    "\n",
    "ep_lens = []\n",
    "ep_rets = []\n",
    "\n",
    "def calculate_reward(observ, torque, target_angle): # Todos los valores estan en radianes\n",
    "\t\ttheta = math.atan2(observ[1],observ[0])\n",
    "\t\ttheta_dot = observ[2]\n",
    "\t\t\n",
    "\t\ttheta_n = ((theta + np.pi) % (2*np.pi)) - np.pi\n",
    "\n",
    "\t\ttheta_error = np.abs(theta_n - target_angle)\n",
    "\t\t\n",
    "\t\t#torque_castigo = (torque**2) - np.minimum(2-np.absolute(torque),0)\n",
    "\t\ttorque_castigo = 0.001 * (torque**2)\n",
    "\t\tcosts = (theta_error**2) + 0.1 * (theta_dot**2) + torque_castigo\n",
    "\t\tif theta_error <= 0.087: # ~ 5Â°\n",
    "\t\t\treward_n = -costs + math.exp(-(8*theta_error)**2)\n",
    "\t\telse:\n",
    "\t\t\treward_n = -costs\n",
    "\t\t# reward_n = -costs\n",
    "\t\treturn reward_n.item()\n",
    "\n",
    "def _log_summary(ep_len, ep_ret, ep_num):\n",
    "\t\t\"\"\"\n",
    "\t\t\tPrint to stdout what we've logged so far in the most recent episode.\n",
    "\n",
    "\t\t\tParameters:\n",
    "\t\t\t\tNone\n",
    "\n",
    "\t\t\tReturn:\n",
    "\t\t\t\tNone\n",
    "\t\t\"\"\"\n",
    "\t\t# Round decimal places for more aesthetic logging messages\n",
    "\t\tep_lena = str(round(ep_len, 2))\n",
    "\t\tep_reta = str(round(ep_ret, 2))\n",
    "\n",
    "\t\t# Print logging statements\n",
    "\t\tprint(flush=True)\n",
    "\t\tprint(f\"-------------------- Episode #{ep_num} --------------------\", flush=True)\n",
    "\t\tprint(f\"Episodic Length: {ep_lena}\", flush=True)\n",
    "\t\tprint(f\"Episodic Return: {ep_reta}\", flush=True)\n",
    "\t\tprint(f\"------------------------------------------------------\", flush=True)\n",
    "\t\tprint(flush=True)\n",
    "  \n",
    "\t\t\n",
    "\t\t# plt.figure(1)\n",
    "\t\t# # Round decimal places for more aesthetic logging messages\n",
    "\t\t# ep_lens = round(ep_len, 2)\n",
    "\t\t# ep_rets = round(ep_ret, 2)\n",
    "\n",
    "\t\t# # Append to lists\n",
    "\t\t# if ep_lens:\n",
    "\t\t# \tep_lens.append(ep_lens[-1] + ep_len)\n",
    "\t\t# else:\n",
    "\t\t# \tep_lens.append(ep_len)\n",
    "\t\t# ep_rets.append(ep_ret)\n",
    "\n",
    "\t\t# # Clear previous plot\n",
    "\t\t# plt.clf()\n",
    "\n",
    "\t\t# # Plot\n",
    "\t\t# plt.plot(ep_lens, ep_rets, color='blue')\n",
    "\t\t# plt.title(f'Episode #{ep_num} - Episodic Length vs Episodic Return')\n",
    "\t\t# plt.xlabel('Episodic Length')\n",
    "\t\t# plt.ylabel('Episodic Return')\n",
    "\t\t# plt.grid(True)\n",
    "\t\t# plt.pause(0.01)\n",
    "\t\t# if is_ipython:\n",
    "\t\t# \tdisplay.display(plt.gcf())\n",
    "\t\t# \tdisplay.clear_output(wait=True)\n",
    "\n",
    "\n",
    "def rollout(policy, env, render, target_angle):\n",
    "\t\"\"\"\n",
    "\t\tReturns a generator to roll out each episode given a trained policy and\n",
    "\t\tenvironment to test on. \n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tpolicy - The trained policy to test\n",
    "\t\t\tenv - The environment to evaluate the policy on\n",
    "\t\t\trender - Specifies whether to render or not\n",
    "\t\t\n",
    "\t\tReturn:\n",
    "\t\t\tA generator object rollout, or iterable, which will return the latest\n",
    "\t\t\tepisodic length and return on each iteration of the generator.\n",
    "\n",
    "\t\tNote:\n",
    "\t\t\tIf you're unfamiliar with Python generators, check this out:\n",
    "\t\t\t\thttps://wiki.python.org/moin/Generators\n",
    "\t\t\tIf you're unfamiliar with Python \"yield\", check this out:\n",
    "\t\t\t\thttps://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n",
    "\t\"\"\"\n",
    "\t# Rollout until user kills process\n",
    "\twhile True:\n",
    "\t\t# obs = env.reset()\t\t\t\t# For Gym version\n",
    "\t\tobs, _ = env.reset()\t\t\t# for Gymnasium version\n",
    "\t\tdone = False\n",
    "\n",
    "\t\t# number of timesteps so far\n",
    "\t\tt = 0\n",
    "\n",
    "\t\t# Logging data\n",
    "\t\tep_len = 0            # episodic length\n",
    "\t\tep_ret = 0            # episodic return\n",
    "\n",
    "\t\twhile not done:\n",
    "\t\t\tt += 1\n",
    "\n",
    "\t\t\t# Render environment if specified, off by default\n",
    "\t\t\tif render:\n",
    "\t\t\t\tenv.render()\n",
    "\n",
    "\t\t\t# Query deterministic action from policy and run it\n",
    "\t\t\taction = policy(obs).detach().cpu().numpy()\n",
    "\t\t\t#obs, rew, done, _ = env.step(action)\t\t\t\t\t# For Gym version\n",
    "\t\t\tobs, rew, terminated, truncated, _ = env.step(action)\t# For Gymnasium version\n",
    "\n",
    "\t\t\tdone = terminated or truncated\t\t\t\t\t\t\t# For Gymnasium\n",
    "\n",
    "\t\t\trew = calculate_reward(obs, action, math.radians(target_angle))\n",
    "\n",
    "\t\t\t# Sum all episodic rewards as we go along\n",
    "\t\t\tep_ret += rew\n",
    "\t\t\t\n",
    "\t\t# Track episodic length\n",
    "\t\tep_len = t\n",
    "\t\t\n",
    "\t\t# returns episodic length and return in this iteration\n",
    "\t\tyield ep_len, ep_ret\n",
    "\n",
    "def eval_policy(policy, env, render=False, target_angle=0):\n",
    "\t\"\"\"\n",
    "\t\tThe main function to evaluate our policy with. It will iterate a generator object\n",
    "\t\t\"rollout\", which will simulate each episode and return the most recent episode's\n",
    "\t\tlength and return. We can then log it right after. And yes, eval_policy will run\n",
    "\t\tforever until you kill the process. \n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tpolicy - The trained policy to test, basically another name for our actor model\n",
    "\t\t\tenv - The environment to test the policy on\n",
    "\t\t\trender - Whether we should render our episodes. False by default.\n",
    "\n",
    "\t\tReturn:\n",
    "\t\t\tNone\n",
    "\n",
    "\t\tNOTE: To learn more about generators, look at rollout's function description\n",
    "\t\"\"\"\n",
    "\t# Rollout with the policy and environment, and log each episode's data\n",
    "\tfor ep_num, (ep_len, ep_ret) in enumerate(rollout(policy, env, render, target_angle)):\n",
    "\t\t_log_summary(ep_len=ep_len, ep_ret=ep_ret, ep_num=ep_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing actor-critic/ppo_actor.pth\n",
      "\n",
      "-------------------- Episode #0 --------------------\n",
      "Episodic Length: 200\n",
      "Episodic Return: -2788.93\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Episode #1 --------------------\n",
      "Episodic Length: 200\n",
      "Episodic Return: -2854.27\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------- Episode #2 --------------------\n",
      "Episodic Length: 200\n",
      "Episodic Return: -2652.54\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 113\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    110\u001b[0m \t\u001b[38;5;66;03m#args = get_args() # Parse arguments from command line # Just for .py files\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \t\u001b[38;5;66;03m# args = argparse.Namespace(mode='train', target_angle='135', actor_model='', critic_model='')\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \targs \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mNamespace(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, target_angle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m135\u001b[39m\u001b[38;5;124m'\u001b[39m, actor_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor-critic/ppo_actor.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, critic_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor-critic/ppo_critic.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m \t\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 106\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    104\u001b[0m \ttrain(env\u001b[38;5;241m=\u001b[39menv, hyperparameters\u001b[38;5;241m=\u001b[39mhyperparameters, target_angle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(args\u001b[38;5;241m.\u001b[39mtarget_angle), actor_model\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mactor_model, critic_model\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mcritic_model)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m \t\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_angle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_angle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[5], line 70\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(env, target_angle, actor_model)\u001b[0m\n\u001b[0;32m     64\u001b[0m policy\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(actor_model))\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Evaluate our policy with a separate module, eval_policy, to demonstrate\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# that once we are done training the model/policy with ppo.py, we no longer need\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# ppo.py since it only contains the training algorithm. The model/policy itself exists\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# independently as a binary file that can be loaded in with torch.\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[43meval_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_angle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_angle\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 161\u001b[0m, in \u001b[0;36meval_policy\u001b[1;34m(policy, env, render, target_angle)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m\tThe main function to evaluate our policy with. It will iterate a generator object\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m\t\"rollout\", which will simulate each episode and return the most recent episode's\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m\tNOTE: To learn more about generators, look at rollout's function description\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Rollout with the policy and environment, and log each episode's data\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep_num, (ep_len, ep_ret) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(rollout(policy, env, render, target_angle)):\n\u001b[0;32m    162\u001b[0m \t_log_summary(ep_len\u001b[38;5;241m=\u001b[39mep_len, ep_ret\u001b[38;5;241m=\u001b[39mep_ret, ep_num\u001b[38;5;241m=\u001b[39mep_num)\n",
      "Cell \u001b[1;32mIn[4], line 123\u001b[0m, in \u001b[0;36mrollout\u001b[1;34m(policy, env, render, target_angle)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Render environment if specified, off by default\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[1;32m--> 123\u001b[0m \t\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Query deterministic action from policy and run it\u001b[39;00m\n\u001b[0;32m    126\u001b[0m action \u001b[38;5;241m=\u001b[39m policy(obs)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gymnasium\\core.py:418\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gymnasium\\wrappers\\env_checker.py:65\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\gymnasium\\envs\\classic_control\\pendulum.py:264\u001b[0m, in \u001b[0;36mPendulumEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# mode == \"rgb_array\":\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m--> 264\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurfarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpixels3d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    265\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(env, hyperparameters, target_angle, actor_model, critic_model):\n",
    "\t\"\"\"\n",
    "\t\tTrains the model.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tenv - the environment to train on\n",
    "\t\t\thyperparameters - a dict of hyperparameters to use, defined in main\n",
    "\t\t\tactor_model - the actor model to load in if we want to continue training\n",
    "\t\t\tcritic_model - the critic model to load in if we want to continue training\n",
    "\n",
    "\t\tReturn:\n",
    "\t\t\tNone\n",
    "\t\"\"\"\t\n",
    "\tprint(f\"Training\", flush=True)\n",
    "\n",
    "\t# Create a model for PPO.\n",
    "\tmodel = PPO(policy_class=FeedForwardNN, env=env, **hyperparameters)\n",
    "\n",
    "\t# Tries to load in an existing actor/critic model to continue training on\n",
    "\tif actor_model != '' and critic_model != '':\n",
    "\t\tprint(f\"Loading in {actor_model} and {critic_model}...\", flush=True)\n",
    "\t\tmodel.actor.load_state_dict(torch.load(actor_model))\n",
    "\t\tmodel.critic.load_state_dict(torch.load(critic_model))\n",
    "\t\tprint(f\"Successfully loaded.\", flush=True)\n",
    "\telif actor_model != '' or critic_model != '': # Don't train from scratch if user accidentally forgets actor/critic model\n",
    "\t\tprint(f\"Error: Either specify both actor/critic models or none at all. We don't want to accidentally override anything!\")\n",
    "\t\tsys.exit(0)\n",
    "\telse:\n",
    "\t\tprint(f\"Training from scratch.\", flush=True)\n",
    "\n",
    "\t# Train the PPO model with a specified total timesteps\n",
    "\t# NOTE: You can change the total timesteps here, I put a big number just because\n",
    "\t# you can kill the process whenever you feel like PPO is converging\n",
    "\t#model.learn(total_timesteps=200_000_000, target_angle=target_angle)\n",
    "\t#model.learn(total_timesteps=1_000_000, target_angle=target_angle)\n",
    "\tmodel.learn(total_timesteps=100_000, target_angle=target_angle)\n",
    "\n",
    "def test(env, target_angle, actor_model):\n",
    "\t\"\"\"\n",
    "\t\tTests the model.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tenv - the environment to test the policy on\n",
    "\t\t\tactor_model - the actor model to load in\n",
    "\n",
    "\t\tReturn:\n",
    "\t\t\tNone\n",
    "\t\"\"\"\n",
    "\tprint(f\"Testing {actor_model}\", flush=True)\n",
    "\n",
    "\t# If the actor model is not specified, then exit\n",
    "\tif actor_model == '':\n",
    "\t\tprint(f\"Didn't specify model file. Exiting.\", flush=True)\n",
    "\t\tsys.exit(0)\n",
    "\n",
    "\t# Extract out dimensions of observation and action spaces\n",
    "\tobs_dim = env.observation_space.shape[0]\n",
    "\tact_dim = env.action_space.shape[0]\n",
    "\n",
    "\t# Build our policy the same way we build our actor model in PPO\n",
    "\tpolicy = FeedForwardNN(obs_dim, act_dim).to(device)\n",
    "\n",
    "\t# Load in the actor model saved by the PPO algorithm\n",
    "\tpolicy.load_state_dict(torch.load(actor_model))\n",
    "\n",
    "\t# Evaluate our policy with a separate module, eval_policy, to demonstrate\n",
    "\t# that once we are done training the model/policy with ppo.py, we no longer need\n",
    "\t# ppo.py since it only contains the training algorithm. The model/policy itself exists\n",
    "\t# independently as a binary file that can be loaded in with torch.\n",
    "\teval_policy(policy=policy, env=env, render=True, target_angle=target_angle)\n",
    "\n",
    "def main(args):\n",
    "\t\"\"\"\n",
    "\t\tThe main function to run.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\targs - the arguments parsed from command line\n",
    "\n",
    "\t\tReturn:\n",
    "\t\t\tNone\n",
    "\t\"\"\"\n",
    "\t# NOTE: Here's where you can set hyperparameters for PPO. I don't include them as part of\n",
    "\t# ArgumentParser because it's too annoying to type them every time at command line. Instead, you can change them here.\n",
    "\t# To see a list of hyperparameters, look in ppo.py at function _init_hyperparameters\n",
    "\thyperparameters = {\n",
    "\t\t\t\t'timesteps_per_batch': 2048, \n",
    "\t\t\t\t'max_timesteps_per_episode': 200, \n",
    "\t\t\t\t'gamma': 0.99, \n",
    "\t\t\t\t'n_updates_per_iteration': 10,\n",
    "\t\t\t\t'lr': 3e-4, \n",
    "\t\t\t\t'clip': 0.2,\n",
    "\t\t\t\t'render': True,\n",
    "\t\t\t\t'render_every_i': 10\n",
    "\t\t\t  }\n",
    "\n",
    "\t# Creates the environment we'll be running. If you want to replace with your own\n",
    "\t# custom environment, note that it must inherit Gym and have both continuous\n",
    "\t# observation and action spaces.\n",
    "\tenv = gym.make('Pendulum-v1', render_mode=\"human\", g=9.81)\n",
    "\t# env = gym.make('Pendulum-v1', render_mode=\"rgb_array\", g=9.81)\n",
    "\n",
    "\t# Train or test, depending on the mode specified\n",
    "\tif args.mode == 'train':\n",
    "\t\ttrain(env=env, hyperparameters=hyperparameters, target_angle=int(args.target_angle), actor_model=args.actor_model, critic_model=args.critic_model)\n",
    "\telse:\n",
    "\t\ttest(env=env, target_angle=int(args.target_angle), actor_model=args.actor_model)\n",
    "\tenv.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t#args = get_args() # Parse arguments from command line # Just for .py files\n",
    "\t# args = argparse.Namespace(mode='train', target_angle='135', actor_model='', critic_model='')\n",
    "\targs = argparse.Namespace(mode='test', target_angle='135', actor_model='actor-critic/ppo_actor.pth', critic_model='actor-critic/ppo_critic.pth')\n",
    "\tmain(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba = np.array([-2.45])\n",
    "\n",
    "print(type(prueba))\n",
    "\n",
    "print(prueba.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forPPO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
